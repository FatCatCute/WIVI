import json
import os

file_path = r"C:\Users\admin\Documents\Counting Activity for CV-CSI\wivi_project\src\wivi32_vscode.ipynb"

# New Config Code
new_config_code = [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dataset Paths\n",
    "DATASET_PATH = \"../data/data_activity\"\n",
    "# Note: We will discover classes dynamically from this folder\n",
    "\n",
    "# Output Paths\n",
    "OUTPUT_BASE_PATH = \"../outputs\"\n",
    "os.makedirs(OUTPUT_BASE_PATH, exist_ok=True)\n",
    "print(f\"Running locally - outputs saved to: {OUTPUT_BASE_PATH}\")\n",
    "\n",
    "# Create output subdirectories\n",
    "MODEL_SAVE_PATH = os.path.join(OUTPUT_BASE_PATH, \"models\")\n",
    "PLOTS_SAVE_PATH = os.path.join(OUTPUT_BASE_PATH, \"plots\")\n",
    "RESULTS_SAVE_PATH = os.path.join(OUTPUT_BASE_PATH, \"results\")\n",
    "HISTORY_SAVE_PATH = os.path.join(OUTPUT_BASE_PATH, \"training_history\")\n",
    "\n",
    "for path in [MODEL_SAVE_PATH, PLOTS_SAVE_PATH, RESULTS_SAVE_PATH, HISTORY_SAVE_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Model Configuration\n",
    "# We will update NUM_CLASSES dynamically in the Dataset class or main loop, \n",
    "# but let's set a default based on your folders (Dung, KeoGhe, etc.)\n",
    "NUM_CLASSES = 8 \n",
    "CSI_LENGTH = 100\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "# Preprocessing\n",
    "HAMPEL_K = 3\n",
    "PATCH_SIZE = 16\n",
    "\n",
    "# Task Configuration\n",
    "TASK = 'classification' # Changed to classification for Activity Recognition\n",
    "\n",
    "# Advanced Options\n",
    "SAVE_BEST_ONLY = True\n",
    "VERBOSE_TRAINING = True\n",
    "PLOT_TRAINING_HISTORY = True\n",
    "SAVE_EVALUATION_PLOTS = True\n",
    "\n",
    "# Splits\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "RANDOM_SEED = 42\n"
]

# New Dataset Class Code
new_dataset_code = [
    "# Custom Dataset Class for Activity Recognition\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split='train', transform=None, task='classification', verbose=True):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.task = task\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # 1. Discover Classes\n",
    "        self.classes = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        if self.verbose and split == 'train':\n",
    "            print(f\"Found {len(self.classes)} classes: {self.classes}\")\n",
    "        \n",
    "        # 2. Collect All Samples\n",
    "        self.samples = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(dataset_path, cls_name)\n",
    "            csi_dir = os.path.join(cls_dir, 'csi')\n",
    "            img_dir = os.path.join(cls_dir, 'images')\n",
    "            \n",
    "            if not os.path.exists(csi_dir) or not os.path.exists(img_dir):\n",
    "                if self.verbose: print(f\"Skipping {cls_name}: missing csi or images folder\")\n",
    "                continue\n",
    "                \n",
    "            # Match files by name (assuming same filename in both folders)\n",
    "            csi_files = sorted([f for f in os.listdir(csi_dir) if f.endswith('.csv') or f.endswith('.txt')])\n",
    "            \n",
    "            for csi_file in csi_files:\n",
    "                # Try to find corresponding image\n",
    "                # Assuming image has same basename. Check extensions.\n",
    "                basename = os.path.splitext(csi_file)[0]\n",
    "                img_file = None\n",
    "                for ext in ['.jpg', '.png', '.jpeg']:\n",
    "                    if os.path.exists(os.path.join(img_dir, basename + ext)):\n",
    "                        img_file = basename + ext\n",
    "                        break\n",
    "                \n",
    "                if img_file:\n",
    "                    self.samples.append({\n",
    "                        'csi_path': os.path.join(csi_dir, csi_file),\n",
    "                        'img_path': os.path.join(img_dir, img_file),\n",
    "                        'label': self.class_to_idx[cls_name],\n",
    "                        'class_name': cls_name\n",
    "                    })\n",
    "        \n",
    "        # 3. Split Data (Simple random split based on index for now, or use the passed 'split' arg)\n",
    "        # Note: In a real scenario, we should split by subject or session to avoid leakage.\n",
    "        # Here we just shuffle and split all samples.\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(len(self.samples))\n",
    "        train_idx = int(len(indices) * 0.8)\n",
    "        val_idx = int(len(indices) * 0.9)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.indices = indices[:train_idx]\n",
    "        elif split == 'val':\n",
    "            self.indices = indices[train_idx:val_idx]\n",
    "        else: # test\n",
    "            self.indices = indices[val_idx:]\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"[{split.upper()}] Loaded {len(self.indices)} samples out of {len(self.samples)} total.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        sample_info = self.samples[real_idx]\n",
    "        \n",
    "        # Load CSI\n",
    "        try:\n",
    "            # Assuming CSI is a CSV with numeric values\n",
    "            csi_df = pd.read_csv(sample_info['csi_path'], header=None)\n",
    "            csi_data = csi_df.values.flatten().tolist()\n",
    "            \n",
    "            # Apply Hampel & Normalize (using global functions defined earlier)\n",
    "            csi_data = hampel_filter(csi_data, K=HAMPEL_K)\n",
    "            csi_data = normalize_csi(csi_data)\n",
    "            \n",
    "            # Pad or truncate to CSI_LENGTH\n",
    "            if len(csi_data) > CSI_LENGTH:\n",
    "                csi_data = csi_data[:CSI_LENGTH]\n",
    "            else:\n",
    "                csi_data = csi_data + [0] * (CSI_LENGTH - len(csi_data))\n",
    "                \n",
    "            csi_tensor = torch.tensor(csi_data, dtype=torch.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSI {sample_info['csi_path']}: {e}\")\n",
    "            csi_tensor = torch.zeros(CSI_LENGTH, dtype=torch.float32)\n",
    "\n",
    "        # Load Image\n",
    "        try:\n",
    "            image = Image.open(sample_info['img_path']).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Image {sample_info['img_path']}: {e}\")\n",
    "            image = torch.zeros((3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "        # Label\n",
    "        label = torch.tensor(sample_info['label'], dtype=torch.long)\n",
    "        \n",
    "        return csi_tensor, image, label\n"
]

with open(file_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Update Cell 3 (Config)
nb['cells'][3]['source'] = new_config_code

# Update Cell 6 (Dataset Class)
# Note: The index might vary if there are markdown cells. 
# Based on previous inspection:
# Cell 3 was Config.
# Cell 4 was Imports.
# Cell 6 was Dataset Class.
nb['cells'][6]['source'] = new_dataset_code

with open(file_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=1)

print("Notebook updated successfully!")
